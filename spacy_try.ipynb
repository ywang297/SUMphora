{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp_full = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpful_reviews= pd.read_pickle('/Users/yishu/Documents/insight/helpful_reviews.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['my', 'your', 'you', 'whole', 'one', 'love','like','tried','great','good','can','could'\n",
    "                   ,'leave','found','maybe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 absorb\n",
      "1 better\n",
      "2 climate\n",
      "3 combination\n",
      "4 film\n",
      "5 greasy\n",
      "6 leaves\n",
      "7 living\n",
      "8 looking\n",
      "9 morning\n",
      "10 night\n"
     ]
    }
   ],
   "source": [
    "processed_docs = helpful_reviews['r_review'].map(preprocess)\n",
    "#processed_docs[:10]\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (4, 1),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 1),\n",
       " (9, 1),\n",
       " (10, 1),\n",
       " (11, 1),\n",
       " (12, 1),\n",
       " (13, 4),\n",
       " (14, 1),\n",
       " (15, 1),\n",
       " (16, 1)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_review_helpful = review_helpful['r_review']\n",
    "docs = [nlp_full(desc) for desc in r_review_helpful]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[I have combination skin that tends to be dry due to living in a dry climate. I found that I could not use this in the morning before the day because it leaves my skin looking greasy. While it did not leave a film, it did not absorb well into my skin. Maybe it would be OK to use at night, but there are better options that will do more for your skin for the price.,\n",
       " Truly a holy grail product. Every time I run out, I get antsy and try something new...and I always regret it and come running back to this cream. It feels so good on skin, sinks in pretty quickly and never pills when layering serums or oils at night. This keeps my skin’s texture so smooth and absolutely softens the appearance of fine lines. I’ve got a few best friends hooked, too.,\n",
       " This cream feels great on your skin, smells great and is super hydrating. Unfortunately it made me break out. And I’m almost 60 years old. So not great for me.,\n",
       " Despite the hefty price tag, I do think this is one of the better moisturizers out there! I first tried a few samples and decided to purchase the 2.5 fl. oz. I use it along with my tretinoin treatment, which really dries out my normally oily skin, especially in the winter months. I spread a very thin amount in the morning before sunscreen and then am a little more generous every other evenings. It helps keep the dry flakes under control, leaves my skin looking smoother and brighter, and most importantly leaves my face supple and moisturized. I personally haven't broken out or faced any sensitivity. I took a break to try other products once I finished it, but I find myself going for it again! Definitely worth it :),\n",
       " It is a nice formula, but it didn’t do what it promises. It didn’t make me break out, but after using it for 2 months it didn’t do much of anything. I’d pass on this one.]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in stop_words and len(token) > 3:\n",
    "            result.append(token)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_preferred_spelling_dict(words):\n",
    "    '''\n",
    "    For some set of words, returns a dict mapping each unique lowercased word to\n",
    "    its most popular spelling and total occurrences of all spellings.\n",
    "    '''\n",
    "    spellings = {}\n",
    "    for word in words:\n",
    "        word_lower = word.lower()\n",
    "        if word_lower in spellings:\n",
    "            spellings[word_lower].append(word)\n",
    "        else:\n",
    "            spellings[word_lower] = [word]\n",
    "    preferred_spellings = {}\n",
    "    for (word, spelling_cands) in spellings.items():\n",
    "        n_occurrences = len(spelling_cands)\n",
    "        preferred_spelling = max(set(spelling_cands), key=spelling_cands.count)\n",
    "        preferred_spellings[word] = (preferred_spelling, n_occurrences)\n",
    "    return preferred_spellings\n",
    "\n",
    "def generate_multiplicity_dict(words):\n",
    "    '''\n",
    "    Counts the number of occurrences of each word, case-sensitive.\n",
    "    '''\n",
    "    multiplicities = {}\n",
    "    for word in words:\n",
    "        if word not in multiplicities:\n",
    "            multiplicities[word] = words.count(word)\n",
    "    return multiplicities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cands_directly_describing_subject(cands, subj_descriptors):\n",
    "    return [cand for cand in cands if cand.lower() in subj_descriptors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highest_multiplicity_cand(cands, preferred_spellings):\n",
    "    return max(cands, key=lambda cand: preferred_spellings[cand.lower()][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "listing_noun_phrases = []\n",
    "stop_tags = ['PRP', 'DT','IN','TO','MD'] #'IN'\n",
    "for doc in docs:\n",
    "    #spans = [span for span in list(doc.noun_chunks) ]\n",
    "    #tokens = [(token, token.tag_) for span in spans for token in span]\n",
    "    #print(tokens)\n",
    "    noun_phrases = []\n",
    "    for np in doc.noun_chunks:\n",
    "        if np.root.tag_ not in stop_tags:\n",
    "            important_descriptors = [word for word in np if not word.tag_ in stop_tags and not word.text == np.root.text]\n",
    "            noun_phrases.append((important_descriptors, np.root.text))\n",
    "    listing_noun_phrases.append(noun_phrases)\n",
    "#listing_noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "listing_noun_phrase_subjects = [np for listing in listing_noun_phrases for (descriptors, np) in listing]\n",
    "subject_preferred_spellings = generate_preferred_spelling_dict(listing_noun_phrase_subjects)\n",
    "popular_descriptors = list(subject_preferred_spellings.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_descriptors.sort(key=lambda desc: desc[1][1], reverse=True)\n",
    "#popular_descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skin:\n",
      "\tmy (4143)\n",
      "\tyour (550)\n",
      "\tdry (426)\n",
      "\tsensitive (331)\n",
      "\toily (324)\n",
      "\tcombination (232)\n",
      "\t, (152)\n",
      "\tvery dry (69)\n",
      "product:\n",
      "\tgreat (133)\n",
      "\tmy (45)\n",
      "\tamazing (36)\n",
      "\tbest (36)\n",
      "\tgood (33)\n",
      "\tfirst (32)\n",
      "\tnew (31)\n",
      "\tonly (26)\n",
      "brush:\n",
      "\tmy (152)\n",
      "\tbest (86)\n",
      "\tgreat (77)\n",
      "\tblush (68)\n",
      "\tfoundation (61)\n",
      "\tpowder (52)\n",
      "\tgood (43)\n",
      "\tlip (36)\n",
      "face:\n",
      "\tmy (1644)\n",
      "\tyour (342)\n",
      "\twhole (45)\n",
      "\tentire (35)\n",
      "\tfull (21)\n",
      "\tdry (12)\n",
      "\ttheir (10)\n",
      "\tsmall (6)\n",
      "scent:\n",
      "\t, (71)\n",
      "\tmy (58)\n",
      "\tlight (57)\n",
      "\tnice (48)\n",
      "\tfresh (41)\n",
      "\tfloral (26)\n",
      "\tbeautiful (26)\n",
      "\tstrong (24)\n",
      "one:\n",
      "\tsecond (24)\n",
      "\tmy (23)\n",
      "\tnew (18)\n",
      "\tbest (11)\n",
      "\tgood (11)\n",
      "\toriginal (10)\n",
      "\tnot (9)\n",
      "\tfirst (8)\n",
      "makeup:\n",
      "\tmy (472)\n",
      "\teye (100)\n",
      "\tyour (76)\n",
      "\tall (47)\n",
      "\tface (20)\n",
      "\twaterproof (16)\n",
      "\tfull (7)\n",
      "\tother (7)\n",
      "cream:\n",
      "\teye (239)\n",
      "\tnight (112)\n",
      "\tBB (87)\n",
      "\tmy (83)\n",
      "\thand (39)\n",
      "\tface (37)\n",
      "\tday (32)\n",
      "\tbest (25)\n",
      "products:\n",
      "\tother (165)\n",
      "\tmy (70)\n",
      "\tfresh (40)\n",
      "\tmany (36)\n",
      "\tPhilosophy (33)\n",
      "\ttheir (31)\n",
      "\tDior (31)\n",
      "\tShiseido (27)\n",
      "time:\n",
      "\tfirst (118)\n",
      "\tlong (109)\n",
      "\tsame (65)\n",
      "\tmy (47)\n",
      "\thard (31)\n",
      "\tnight (19)\n",
      "\tvery long (17)\n",
      "\tmore (16)\n"
     ]
    }
   ],
   "source": [
    "most_popular_descriptors = [descriptor for (descriptor, _) in popular_descriptors[:10]]\n",
    "aggregate_indirect_descriptors = []\n",
    "indirect_descriptor_phrases = {descriptor:[] for descriptor in most_popular_descriptors}\n",
    "for listing in listing_noun_phrases:\n",
    "    for descriptors, subject in listing:\n",
    "        subject_lower = subject.lower()\n",
    "        if subject_lower in most_popular_descriptors:\n",
    "            subject_descriptions = []\n",
    "            description_buffer = []\n",
    "            for descriptor in descriptors:\n",
    "                #if len(descriptor.text) == 1 and re.findall('[^A-Za-z0-9]', descriptor.text): continue\n",
    "                description_buffer.append(descriptor.text)\n",
    "                aggregate_indirect_descriptors.append(descriptor.text)\n",
    "                #print(descriptor.text)\n",
    "                # If the descriptor directly modifies the subject of the NP, take it\n",
    "                # and all descriptors in the buffer (that presumably modify this new descriptor)\n",
    "                if descriptor.head.text == subject:\n",
    "                    subject_descriptions.append(description_buffer)\n",
    "                    description_buffer = []\n",
    "            indirect_descriptor_phrases[subject_lower].append(subject_descriptions)\n",
    "            \n",
    "            #print(subject)        \n",
    "            #print(subject_descriptions)\n",
    "            \n",
    "preferred_descriptor_spellings = generate_preferred_spelling_dict(aggregate_indirect_descriptors)\n",
    "#print(indirect_descriptor_phrases)\n",
    "# for subject, listings in indirect_descriptor_phrases.items():\n",
    "#     for i, listing in enumerate(listings):\n",
    "#         for j, description in enumerate(listing):\n",
    "#             print(description)\n",
    "#             for k, descriptor in enumerate(description):\n",
    "#                 #print(descriptor)\n",
    "#                 preferred_spelling = preferred_descriptor_spellings[descriptor.lower()][0]\n",
    "#                 if descriptor != preferred_spelling:\n",
    "#                     indirect_descriptor_phrases[subject][i][j][k] = preferred_spelling\n",
    "# print(indirect_descriptor_phrases)\n",
    "top_indirect_descriptors = {descriptor:[] for descriptor in most_popular_descriptors}\n",
    "for subject, listings in indirect_descriptor_phrases.items():\n",
    "    flattened_indirect_descriptor_phrase_list = []\n",
    "    for listing in listings:\n",
    "        for description in listing:\n",
    "            # This will unfortunately put spaces around hyphens, and that sort of thing\n",
    "            text_description = ' '.join([preferred_descriptor_spellings[descriptor.lower()][0] for descriptor in description])\n",
    "            flattened_indirect_descriptor_phrase_list.append(text_description)\n",
    "    preferred_descriptions = list(generate_multiplicity_dict(flattened_indirect_descriptor_phrase_list).items())\n",
    "    preferred_descriptions.sort(key=lambda desc: desc[1], reverse=True)\n",
    "    top_indirect_descriptors[subject] = preferred_descriptions\n",
    "\n",
    "for feature, descriptors in top_indirect_descriptors.items():\n",
    "    print(f'{feature}:')\n",
    "    for descriptor, mult in descriptors[:8]:\n",
    "        print(f'\\t{descriptor} ({mult})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
